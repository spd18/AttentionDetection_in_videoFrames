{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7d1401-6e17-4c09-8d48-9fa63669cd07",
   "metadata": {},
   "source": [
    "This file is responsible to create and assign Label to the frame that we extracted from the videos.\n",
    "\n",
    "**Create a JSON file**, each JSON object will contain\n",
    "- Imagefile name\n",
    "- Facial coordinates\n",
    "- Eye Gazing coordinates\n",
    "- emotion\n",
    "- emotion_confidence\n",
    "- is_drowsy\n",
    "- gazing_at_screen\n",
    "- attention_score\n",
    "- head_direction\n",
    "- eye_direction\n",
    "- computer_label\n",
    "\n",
    "**Create a CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0dc91-bc66-4737-bb8d-dcfb73054bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747f340-c79c-4003-9dc2-cd63b0be49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09412e94-d9dc-462b-a768-57fe747f0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01defa-aee9-469b-8562-887c50fb44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773572ac-bd70-496d-a578-447d47ef32a1",
   "metadata": {},
   "source": [
    "Need to upload zip file that contains all the frames, then we need to upzip that zip file.\n",
    "The below code extracts the content of zip file and create a folder name dataset which contains all the extracted file which is extracted frame from videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ab65a-28a5-48d1-bc4f-ad5f433f57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Get the uploaded filename\n",
    "zip_filename = './All_extracted_frames.zip' #next(iter(uploaded))\n",
    "\n",
    "# Unzip to a specific directory (creates if doesn't exist)\n",
    "extract_path = './dataset'\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Unzipped {zip_filename} to {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92004f94-71cc-42b7-afdd-913e77818d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"./dataset/All_extracted_frames/\"  \n",
    "\n",
    "# Count image files\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "image_count = len([f for f in os.listdir(folder_path) if f.lower().endswith(image_extensions)])\n",
    "\n",
    "print(f\"Total images in '{folder_path}': {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdbf07-15c6-40d7-b58c-a581d148db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,\n",
    "                                 max_num_faces=1,\n",
    "                                 refine_landmarks=True)\n",
    "\n",
    "# Reference 3D model for head pose estimation\n",
    "FACE_3D = np.array([\n",
    "    (0.0, 0.0, 0.0),\n",
    "    (0.0, -330.0, -65.0),\n",
    "    (-225.0, 170.0, -135.0),\n",
    "    (225.0, 170.0, -135.0),\n",
    "    (-150.0, -150.0, -125.0),\n",
    "    (150.0, -150.0, -125.0)\n",
    "], dtype=np.float64)\n",
    "LANDMARK_IDS = [1, 152, 33, 263, 61, 291]\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "# Configuration\n",
    "image_folder = './dataset/All_extracted_frames/'\n",
    "output_json = 'all_attention_results.json'\n",
    "max_images = 2400  # we can define number of frames to be extracted from all the videos, this will automatically calculated the frames to be extracted from all videos.\n",
    "\n",
    "# Drowsiness detection parameters\n",
    "EYE_AR_THRESH = 0.25  # Eye aspect ratio threshold\n",
    "\n",
    "def detect_emotion(face_roi):\n",
    "    \"\"\"Detect emotion using DeepFace with proper error handling\"\"\"\n",
    "    try:\n",
    "        face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        analysis = DeepFace.analyze(face_rgb, actions=['emotion'], enforce_detection=False, silent=True)\n",
    "        if analysis and isinstance(analysis, list):\n",
    "            emotion = analysis[0]['dominant_emotion']\n",
    "            confidence = float(analysis[0]['emotion'][emotion])/100  # Convert to float\n",
    "            return emotion, confidence\n",
    "        return None, 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion detection error: {str(e)}\")\n",
    "        return None, 0.0\n",
    "\n",
    "def is_drowsy(eye_aspect_ratio):\n",
    "    \"\"\"Determine if person is drowsy\"\"\"\n",
    "    return eye_aspect_ratio < EYE_AR_THRESH if eye_aspect_ratio else False\n",
    "\n",
    "def calculate_attention_score(features):\n",
    "    \"\"\"Calculate comprehensive attention score (0-1)\"\"\"\n",
    "    if not features.get('face'):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    # Face orientation (30% weight)\n",
    "    if features['face'].get('is_frontal', False):\n",
    "        score += 0.3\n",
    "\n",
    "    # Eye gaze (25% weight)\n",
    "    if features.get('gazing_at_screen', False):\n",
    "        score += 0.25\n",
    "\n",
    "    # Emotion (10% weight)\n",
    "    emotion = features.get('emotion')\n",
    "    if emotion in ['happy', 'neutral', 'surprise']:\n",
    "        score += 0.1\n",
    "    elif emotion in ['angry', 'disgust', 'fear', 'sad']:\n",
    "        score -= 0.05\n",
    "\n",
    "    # Drowsiness (25% weight)\n",
    "    if not features.get('is_drowsy', True):\n",
    "        score += 0.25\n",
    "\n",
    "    return float(max(0.0, min(1.0, score)))\n",
    "\n",
    "def detect_face_and_gaze(image):\n",
    "    \"\"\"Enhanced face and gaze detection using MediaPipe\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    face_data = {\n",
    "        'box': None,\n",
    "        'is_frontal': False,\n",
    "        'gazing_at_screen': False,\n",
    "        'eyes': [],\n",
    "        'head_direction': None,\n",
    "        'eye_direction': None,\n",
    "        'drowsy': False\n",
    "    }\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        mesh = result.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # Head pose estimation\n",
    "        image_points = np.array([(int(mesh[i].x * w), int(mesh[i].y * h)) for i in LANDMARK_IDS], dtype=\"double\")\n",
    "        cam_matrix = np.array([[w, 0, w / 2],\n",
    "                             [0, w, h / 2],\n",
    "                             [0, 0, 1]])\n",
    "        dist_coeffs = np.zeros((4, 1))\n",
    "        success, rvec, tvec = cv2.solvePnP(FACE_3D, image_points, cam_matrix, dist_coeffs)\n",
    "        rmat, _ = cv2.Rodrigues(rvec)\n",
    "        angles, _, _, _, _, _ = cv2.RQDecomp3x3(rmat)\n",
    "        yaw = angles[1]  # horizontal head rotation\n",
    "\n",
    "        # Face direction classification\n",
    "        if yaw < -15:\n",
    "            face_data['head_direction'] = \"left\"\n",
    "            face_data['is_frontal'] = False\n",
    "        elif yaw > 15:\n",
    "            face_data['head_direction'] = \"right\"\n",
    "            face_data['is_frontal'] = False\n",
    "        else:\n",
    "            face_data['head_direction'] = \"front\"\n",
    "            face_data['is_frontal'] = True\n",
    "\n",
    "        # Get face bounding box\n",
    "        x_coords = [int(lm.x * w) for lm in mesh]\n",
    "        y_coords = [int(lm.y * h) for lm in mesh]\n",
    "        face_data['box'] = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n",
    "\n",
    "        # Iris-based eye direction\n",
    "        left_iris = mesh[468]\n",
    "        right_iris = mesh[473]\n",
    "        iris_x = (left_iris.x + right_iris.x) / 2 * w\n",
    "        face_center_x = (mesh[33].x + mesh[263].x) / 2 * w\n",
    "        gaze_offset = iris_x - face_center_x\n",
    "\n",
    "        if gaze_offset < -30:\n",
    "            face_data['eye_direction'] = \"left\"\n",
    "            face_data['gazing_at_screen'] = False\n",
    "        elif gaze_offset > 30:\n",
    "            face_data['eye_direction'] = \"right\"\n",
    "            face_data['gazing_at_screen'] = False\n",
    "        else:\n",
    "            face_data['eye_direction'] = \"center\"\n",
    "            face_data['gazing_at_screen'] = True\n",
    "\n",
    "        # Eye boxes and drowsiness detection\n",
    "        for eye_id in [(33, 133), (362, 263)]:  # Left, Right eyes\n",
    "            x1 = int(min(mesh[eye_id[0]].x, mesh[eye_id[1]].x) * w)\n",
    "            y1 = int(min(mesh[eye_id[0]].y, mesh[eye_id[1]].y) * h)\n",
    "            x2 = int(max(mesh[eye_id[0]].x, mesh[eye_id[1]].x) * w)\n",
    "            y2 = int(max(mesh[eye_id[0]].y, mesh[eye_id[1]].y) * h)\n",
    "\n",
    "            eye_data = {\n",
    "                'x': x1,\n",
    "                'y': y1,\n",
    "                'width': x2 - x1,\n",
    "                'height': y2 - y1,\n",
    "                'center': ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            }\n",
    "            face_data['eyes'].append(eye_data)\n",
    "\n",
    "        # Drowsiness detection using eye aspect ratio\n",
    "        if len(face_data['eyes']) >= 2:\n",
    "            eye1 = face_data['eyes'][0]\n",
    "            eye2 = face_data['eyes'][1]\n",
    "            eye_dist = abs(eye1['center'][0] - eye2['center'][0])\n",
    "            if eye_dist > 0:\n",
    "                eye_ar = float((eye1['height'] + eye2['height']) / (2 * eye_dist))\n",
    "                face_data['drowsy'] = is_drowsy(eye_ar)\n",
    "\n",
    "    return face_data\n",
    "\n",
    "results = []\n",
    "\n",
    "for count, filename in enumerate(os.listdir(image_folder)):\n",
    "    if count >= max_images:\n",
    "        break\n",
    "\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(image_folder, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read image: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize features with default values\n",
    "    features = {\n",
    "        'image_name': filename,\n",
    "        'face': None,\n",
    "        'eyes': [],\n",
    "        'emotion': None,\n",
    "        'emotion_confidence': 0.0,\n",
    "        'is_drowsy': False,\n",
    "        'gazing_at_screen': False,\n",
    "        'attention_score': 0.0,\n",
    "        'head_direction': None,\n",
    "        'eye_direction': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Detect face and gaze using MediaPipe\n",
    "        face_data = detect_face_and_gaze(img)\n",
    "\n",
    "        if face_data['box']:\n",
    "            x1, y1, x2, y2 = face_data['box']\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            # Store face features\n",
    "            features['face'] = {\n",
    "                'x': x1,\n",
    "                'y': y1,\n",
    "                'width': w,\n",
    "                'height': h,\n",
    "                'is_frontal': face_data['is_frontal']\n",
    "            }\n",
    "\n",
    "            features['eyes'] = face_data['eyes']\n",
    "            features['gazing_at_screen'] = face_data['gazing_at_screen']\n",
    "            features['is_drowsy'] = face_data['drowsy']\n",
    "            features['head_direction'] = face_data['head_direction']\n",
    "            features['eye_direction'] = face_data['eye_direction']\n",
    "\n",
    "            # Emotion detection\n",
    "            face_roi = img[y1:y2, x1:x2]\n",
    "            emotion, confidence = detect_emotion(face_roi)\n",
    "            features['emotion'] = emotion\n",
    "            features['emotion_confidence'] = confidence\n",
    "\n",
    "            # Calculate attention score\n",
    "            features['attention_score'] = calculate_attention_score(features)\n",
    "\n",
    "            # # Label:\n",
    "            if features['attention_score'] >= 0.75:\n",
    "              features['computer_label'] = 'Attentive'\n",
    "            else:\n",
    "              features['computer_label'] = 'Not Attentive'\n",
    "\n",
    "            # if features['attention_score'] >= 0.8:\n",
    "            #   feature['computer_label'] = 'Highly Attentive'\n",
    "            # elif features['attention_score'] >= 0.6:\n",
    "            #   feature['computer_label'] = 'Moderately Attentive'\n",
    "            # elif features['attention_score'] >= 0.4:\n",
    "            #   feature['computer_label'] = 'Slightly Attentive'\n",
    "            # else:\n",
    "            #   feature['computer_label'] = 'Not Attentive'\n",
    "\n",
    "        # Add to results (with type conversion)\n",
    "        results.append(convert_to_serializable(features.copy()))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        results.append(convert_to_serializable(features))\n",
    "\n",
    "# Save results\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump([r for r in results if r is not None], f, indent=2, default=str)\n",
    "\n",
    "print(f\"Analysis complete! Processed {len([r for r in results if r.get('face')])} faces with {len(results)} total entries.\")\n",
    "print(f\"Results saved to {output_json}\")\n",
    "if results:\n",
    "    valid_results = [r for r in results if r.get('face')]\n",
    "    sample = valid_results[0] if valid_results else results[0]\n",
    "    print(\"\\nSample output:\")\n",
    "    print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847a37a-8284-494a-9f6c-2fb5598b9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code stores extracted features from an image like Head position, Eye detection, classification in drowsy or not drowsy all those feature the defines whether the person is sleepy or not sleepy.\n",
    "import json\n",
    "\n",
    "def update_json_records(filepath):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, updates records with computer_label based on attention_score,\n",
    "    and saves the updated data back to the same file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the JSON file in read mode\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Check if the loaded data is a list (multiple records)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"Successfully loaded {len(data)} records from '{filepath}'\")\n",
    "\n",
    "            # Update each record\n",
    "            updated_count = 0\n",
    "            for record in data:\n",
    "                if 'attention_score' in record:\n",
    "                    # Add computer_label based on attention_score\n",
    "                    if record['attention_score'] >= 0.50:\n",
    "                        record['computer_label'] = 'Attentive'\n",
    "                    else:\n",
    "                        record['computer_label'] = 'Not Attentive'\n",
    "                    updated_count += 1\n",
    "\n",
    "            print(f\"Updated {updated_count} records with computer_label\")\n",
    "\n",
    "            # Save the updated data back to the file\n",
    "            with open(filepath, 'w') as file:\n",
    "                json.dump(data, file, indent=2)\n",
    "            print(f\"Successfully saved updated records to '{filepath}'\")\n",
    "\n",
    "            # Print sample records (first 3)\n",
    "            print(\"\\nSample updated records:\")\n",
    "            for i, record in enumerate(data[:50]):\n",
    "                print(f\"\\n--- Record {i+1} ---\")\n",
    "                print(record)\n",
    "\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"The JSON file contains a single record (dictionary).\")\n",
    "            if 'attention_score' in data:\n",
    "                if data['attention_score'] >= 0.50:\n",
    "                    data['computer_label'] = 'Attentive'\n",
    "                else:\n",
    "                    data['computer_label'] = 'Not Attentive'\n",
    "\n",
    "                # Save the updated data back to the file\n",
    "                with open(filepath, 'w') as file:\n",
    "                    json.dump(data, file, indent=2)\n",
    "                print(f\"Updated single record with computer_label and saved to '{filepath}'\")\n",
    "                print(\"\\nUpdated record:\")\n",
    "                print(data)\n",
    "            else:\n",
    "                print(\"Record doesn't contain attention_score - no updates made\")\n",
    "        else:\n",
    "            print(f\"The JSON file '{filepath}' contains data of an unexpected type: {type(data)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{filepath}'. Check file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_name = \"./all_attention_results.json\"\n",
    "    update_json_records(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7db47-892a-421c-8356-3a2931adf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code converts the JSON file to CSV file.\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "json_file = \"all_attention_results.json\"\n",
    "folder_path = \"./dataset/All_extracted_frames/\"\n",
    "output_csv = \"all_attention_results.csv\"\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create CSV file\n",
    "with open(output_csv, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow([\n",
    "        'Serial No',\n",
    "        'Image Name',\n",
    "        'Computer Label',\n",
    "        'Image Path',\n",
    "        # 'Image (Base64)'\n",
    "    ])\n",
    "\n",
    "    # Process each entry\n",
    "    for idx, entry in enumerate(data, 1):\n",
    "        image_name = entry['image_name']\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        # Get computer label (default to \"Unknown\" if not present)\n",
    "        computer_label = entry.get('computer_label', 'Unknown')\n",
    "\n",
    "        # # Read and encode image\n",
    "        # try:\n",
    "        #     with Image.open(image_path) as img:\n",
    "        #         # Convert image to base64\n",
    "        #         img_buffer = img.tobytes()\n",
    "        #         img_base64 = base64.b64encode(img_buffer).decode('utf-8')\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error processing image {image_name}: {str(e)}\")\n",
    "        #     img_base64 = \"\"\n",
    "\n",
    "        # Write row to CSV\n",
    "        writer.writerow([\n",
    "            idx,                          # Serial No\n",
    "            image_name,                   # Image Name\n",
    "            computer_label,               # Computer Label\n",
    "            image_path,                   # Image Path\n",
    "            # img_base64                    # Base64 encoded image\n",
    "        ])\n",
    "\n",
    "print(f\"CSV file created successfully: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f399705-e022-4845-b1d2-bb0a3830206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code help us to visualize the image based on the detected feature along with matrix that defines \"Attentive or Not Attentive\"\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "results_file = 'all_attention_results.json'\n",
    "image_folder = './dataset/All_extracted_frames/'\n",
    "output_folder = './visualized_results/'\n",
    "num_images_to_visualize = 25  # Number of random images to visualize\n",
    "display_first_n = 25  # Number of images to display in notebook\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Color definitions\n",
    "COLORS = {\n",
    "    'face': (0, 255, 255),        # Yellow\n",
    "    'eyes': (0, 255, 0),          # Green\n",
    "    'gaze_line': (255, 0, 255),   # Purple\n",
    "    'text': (255, 255, 255),      # White\n",
    "    'attention_high': (0, 255, 0), # Green\n",
    "    'attention_med': (255, 255, 0),# Yellow\n",
    "    'attention_low': (255, 0, 0),  # Red\n",
    "    'metric_label': (200, 200, 0), # Light yellow\n",
    "    'metric_value': (200, 255, 255) # Light cyan\n",
    "}\n",
    "\n",
    "# Load results\n",
    "with open(results_file) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Create a dictionary mapping image names to their results\n",
    "result_dict = {result['image_name']: result for result in results}\n",
    "\n",
    "# Get list of all available images\n",
    "all_images = [f for f in os.listdir(image_folder)\n",
    "             if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Filter to only images that have analysis results\n",
    "valid_images = [img for img in all_images if img in result_dict]\n",
    "\n",
    "# Randomly select images\n",
    "random.seed(42)  # For reproducibility\n",
    "selected_images = random.sample(valid_images, min(num_images_to_visualize, len(valid_images)))\n",
    "\n",
    "def visualize_result(result):\n",
    "    \"\"\"Draw visualization with all metrics on the image\"\"\"\n",
    "    img_path = os.path.join(image_folder, result['image_name'])\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read image: {result['image_name']}\")\n",
    "        return None\n",
    "\n",
    "    # Convert to RGB for matplotlib\n",
    "    img_viz = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = img_viz.shape[:2]\n",
    "\n",
    "    # Draw face bounding box\n",
    "    if result.get('face'):\n",
    "        face = result['face']\n",
    "        cv2.rectangle(img_viz,\n",
    "                     (face['x'], face['y']),\n",
    "                     (face['x'] + face['width'], face['y'] + face['height']),\n",
    "                     COLORS['face'], 2)\n",
    "\n",
    "    # Draw eyes and gaze line\n",
    "    if result.get('eyes') and len(result['eyes']) >= 2:\n",
    "        eye1 = result['eyes'][0]\n",
    "        eye2 = result['eyes'][1]\n",
    "\n",
    "        # Eye bounding boxes\n",
    "        for eye in result['eyes']:\n",
    "            cv2.rectangle(img_viz,\n",
    "                         (eye['x'], eye['y']),\n",
    "                         (eye['x'] + eye['width'], eye['y'] + eye['height']),\n",
    "                         COLORS['eyes'], 1)\n",
    "\n",
    "        # Gaze line\n",
    "        cv2.line(img_viz,\n",
    "                (eye1['center'][0], eye1['center'][1]),\n",
    "                (eye2['center'][0], eye2['center'][1]),\n",
    "                COLORS['gaze_line'], 1)\n",
    "\n",
    "    # Create metrics panel\n",
    "    y_start = 30\n",
    "    line_height = 25\n",
    "    panel_width = 300\n",
    "\n",
    "    # Draw semi-transparent panel\n",
    "    overlay = img_viz.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (panel_width, 300), (50, 50, 50), -1)\n",
    "    cv2.addWeighted(overlay, 0.7, img_viz, 0.3, 0, img_viz)\n",
    "\n",
    "    # Display all metrics\n",
    "    def put_metric(label, value, y_pos, value_color=COLORS['metric_value']):\n",
    "        cv2.putText(img_viz, f\"{label}:\",\n",
    "                   (15, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS['metric_label'], 1)\n",
    "        cv2.putText(img_viz, f\"{value}\",\n",
    "                   (150, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, value_color, 1)\n",
    "\n",
    "    y_pos = y_start\n",
    "\n",
    "    # Face metrics\n",
    "    if result.get('face'):\n",
    "        face = result['face']\n",
    "        put_metric(\"Face Position\", f\"{face['x']}, {face['y']}\", y_pos)\n",
    "        y_pos += line_height\n",
    "        put_metric(\"Face Size\", f\"{face['width']}x{face['height']}\", y_pos)\n",
    "        y_pos += line_height\n",
    "        put_metric(\"Frontal Face\", str(face['is_frontal']), y_pos)\n",
    "        y_pos += line_height\n",
    "\n",
    "    # Eye metrics\n",
    "    if result.get('eyes'):\n",
    "        put_metric(\"Eyes Detected\", str(len(result['eyes'])), y_pos)\n",
    "        y_pos += line_height\n",
    "        if len(result['eyes']) >= 2:\n",
    "            put_metric(\"Gazing at Screen\", str(result.get('gazing_at_screen', False)), y_pos)\n",
    "            y_pos += line_height\n",
    "\n",
    "    # Emotion metrics\n",
    "    if result.get('emotion'):\n",
    "        put_metric(\"Emotion\", result['emotion'], y_pos)\n",
    "        y_pos += line_height\n",
    "        put_metric(\"Confidence\", f\"{result['emotion_confidence']*100:.1f}%\", y_pos)\n",
    "        y_pos += line_height\n",
    "\n",
    "    # Drowsiness\n",
    "    if 'is_drowsy' in result:\n",
    "        drowsy_color = COLORS['attention_low'] if result['is_drowsy'] else COLORS['attention_high']\n",
    "        put_metric(\"Drowsy\", str(result['is_drowsy']), y_pos, drowsy_color)\n",
    "        y_pos += line_height\n",
    "\n",
    "    # Attention score\n",
    "    if 'attention_score' in result:\n",
    "        attention_color = COLORS['attention_high'] if result['attention_score'] > 0.7 else \\\n",
    "                         COLORS['attention_med'] if result['attention_score'] > 0.4 else \\\n",
    "                         COLORS['attention_low']\n",
    "        put_metric(\"Attention Score\", f\"{result['attention_score']:.2f}\", y_pos, attention_color)\n",
    "        y_pos += line_height\n",
    "\n",
    "    # Computer label\n",
    "    if 'computer_label' in result:\n",
    "        label_color = COLORS['attention_high'] if result['computer_label'] == 'Attentive' else COLORS['attention_low']\n",
    "        put_metric(\"Computer Label\", result['computer_label'], y_pos, label_color)\n",
    "        y_pos += line_height\n",
    "\n",
    "    return img_viz\n",
    "\n",
    "# Process selected images\n",
    "print(f\"Selected {len(selected_images)} random images for visualization\")\n",
    "for i, img_name in enumerate(selected_images, 1):\n",
    "    result = result_dict[img_name]\n",
    "    print(f\"\\nProcessing {i}/{len(selected_images)}: {img_name}\")\n",
    "\n",
    "    img_viz = visualize_result(result)\n",
    "    if img_viz is None:\n",
    "        continue\n",
    "\n",
    "    # Save visualization\n",
    "    output_path = os.path.join(output_folder, f\"viz_{img_name}\")\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(img_viz, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Display first few results\n",
    "    if i <= display_first_n:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(img_viz)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Analysis Visualization: {img_name}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Visualization complete! Processed {len(selected_images)} random images\")\n",
    "print(f\"Saved visualizations to {output_folder}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
